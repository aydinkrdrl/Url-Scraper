import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import pandas as pd
from collections import defaultdict
import re

# Sayfa konfigÃ¼rasyonu
st.set_page_config(
    page_title="Web Site TarayÄ±cÄ±",
    page_icon="ğŸ•·ï¸",
    layout="wide"
)

class WebScraper:
    def __init__(self, domain):
        self.domain = domain
        self.base_url = f"https://{domain}" if not domain.startswith('http') else domain
        self.visited_urls = set()
        self.all_links = set()
        self.file_extensions = defaultdict(int)
        
    def is_valid_url(self, url):
        """URL'nin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and bool(parsed.scheme)
        except:
            return False
    
    def is_same_domain(self, url):
        """URL'nin aynÄ± domain'e ait olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
        try:
            parsed_url = urlparse(url)
            parsed_base = urlparse(self.base_url)
            return parsed_url.netloc == parsed_base.netloc or parsed_url.netloc.endswith(f".{parsed_base.netloc}")
        except:
            return False
    
    def get_file_extension(self, url):
        """URL'den dosya uzantÄ±sÄ±nÄ± Ã§Ä±kar"""
        try:
            path = urlparse(url).path
            if '.' in path:
                ext = path.split('.')[-1].lower()
                # Sadece yaygÄ±n dosya uzantÄ±larÄ±nÄ± kabul et
                if len(ext) <= 5 and ext.isalnum():
                    return f".{ext}"
        except:
            pass
        return None
    
    def scrape_page(self, url, max_depth=2, current_depth=0):
        """SayfayÄ± tara ve linkleri topla"""
        if current_depth > max_depth or url in self.visited_urls:
            return
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30, verify=False)
            response.raise_for_status()
            
            self.visited_urls.add(url)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # TÃ¼m linkleri bul
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                full_url = urljoin(url, href)
                
                if self.is_valid_url(full_url) and self.is_same_domain(full_url):
                    self.all_links.add(full_url)
                    
                    # Dosya uzantÄ±sÄ±nÄ± kontrol et
                    ext = self.get_file_extension(full_url)
                    if ext:
                        self.file_extensions[ext] += 1
                    
                    # Recursive tarama (sadece HTML sayfalar iÃ§in)
                    if not ext and current_depth < max_depth:
                        time.sleep(0.5)  # Rate limiting
                        self.scrape_page(full_url, max_depth, current_depth + 1)
                        
        except Exception as e:
            st.warning(f"Hata: {url} - {str(e)}")
    
    def start_scraping(self, max_depth=2):
        """TaramayÄ± baÅŸlat"""
        self.scrape_page(self.base_url, max_depth)
        return self.all_links, self.file_extensions

def main():
    st.title("ğŸ•·ï¸ Web Site TarayÄ±cÄ±")
    st.markdown("Bir web sitesindeki tÃ¼m linkleri tarayÄ±n ve dosya tÃ¼rlerine gÃ¶re filtreleyin.")
    
    # Sidebar
    st.sidebar.header("âš™ï¸ Ayarlar")
    
    # Domain input
    domain = st.text_input(
        "ğŸŒ Domain adresini girin:",
        placeholder="example.com veya https://example.com",
        help="Taramak istediÄŸiniz web sitesinin domain adresini girin"
    )
    
    # Tarama derinliÄŸi
    max_depth = st.sidebar.slider("ğŸ“Š Tarama DerinliÄŸi", 1, 3, 2)
    
    if domain:
        if st.button("ğŸš€ TaramayÄ± BaÅŸlat", type="primary"):
            with st.spinner("Site taranÄ±yor... Bu iÅŸlem biraz zaman alabilir."):
                try:
                    scraper = WebScraper(domain)
                    all_links, file_extensions = scraper.start_scraping(max_depth)
                    
                    if all_links:
                        st.success(f"âœ… Tarama tamamlandÄ±! {len(all_links)} link bulundu.")
                        
                        # Dosya uzantÄ±larÄ± seÃ§imi
                        st.subheader("ğŸ“ Dosya TÃ¼rÃ¼ Filtreleme")
                        
                        if file_extensions:
                            st.write("Bulunan dosya tÃ¼rleri:")
                            
                            # Checkbox'lar iÃ§in kolonlar
                            cols = st.columns(4)
                            selected_extensions = []
                            
                            for i, (ext, count) in enumerate(file_extensions.items()):
                                with cols[i % 4]:
                                    if st.checkbox(f"{ext} ({count})", value=True):
                                        selected_extensions.append(ext)
                        else:
                            selected_extensions = []
                            st.info("Ã–zel dosya tÃ¼rÃ¼ bulunamadÄ±.")
                        
                        # HTML sayfalarÄ± da dahil et seÃ§eneÄŸi
                        include_html = st.checkbox("ğŸ“„ HTML sayfalarÄ±nÄ± dahil et", value=True)
                        
                        # SonuÃ§larÄ± filtrele
                        filtered_links = []
                        for link in all_links:
                            ext = scraper.get_file_extension(link)
                            
                            if ext and ext in selected_extensions:
                                filtered_links.append(link)
                            elif not ext and include_html:
                                filtered_links.append(link)
                        
                        # SonuÃ§larÄ± gÃ¶ster
                        st.subheader("ğŸ”— Bulunan Linkler")
                        st.write(f"Toplam: {len(filtered_links)} link")
                        
                        if filtered_links:
                            # DataFrame oluÅŸtur
                            df_data = []
                            for link in filtered_links:
                                ext = scraper.get_file_extension(link) or "HTML"
                                df_data.append({"URL": link, "TÃ¼r": ext})
                            
                            df = pd.DataFrame(df_data)
                            
                            # Tablo gÃ¶ster
                            st.dataframe(df, use_container_width=True)
                            
                            # Ä°ndirme butonu
                            csv = df.to_csv(index=False)
                            st.download_button(
                                label="ğŸ“¥ CSV olarak indir",
                                data=csv,
                                file_name=f"{domain}_links.csv",
                                mime="text/csv"
                            )
                        else:
                            st.warning("SeÃ§ilen kriterlere uygun link bulunamadÄ±.")
                    
                    else:
                        st.error("âŒ HiÃ§ link bulunamadÄ±. Domain adresini kontrol edin.")
                        
                except Exception as e:
                    st.error(f"âŒ Hata oluÅŸtu: {str(e)}")
    
    # Footer
    st.markdown("---")
    st.markdown("ğŸ’¡ **Ä°pucu:** BÃ¼yÃ¼k siteler iÃ§in tarama uzun sÃ¼rebilir. SabÄ±rlÄ± olun!")

if __name__ == "__main__":

    main()
